{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4336b1d",
   "metadata": {},
   "source": [
    "## Data Lake vs Data Lakehouse: Implementação prática com Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0114700",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![capa](images/capa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5a4342",
   "metadata": {},
   "source": [
    "### Evolução das Arquiteturas de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2042bcc6",
   "metadata": {},
   "source": [
    "![data_architectures](images/data_architectures.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dbfde8",
   "metadata": {},
   "source": [
    "> # **Data Lakehouse** é uma nova arquitetura de gerenciamento de dados aberta que implementa estruturas de dados e recursos de gerenciamento de dados semelhantes aos de um Data Warehouse, diretamente no tipo de armazenamento de baixo custo usado para Data Lakes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f76297",
   "metadata": {},
   "source": [
    "![acid](images/acid.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b647361e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\n",
      "Collecting pyspark==3.0.0\n",
      "  Downloading pyspark-3.0.0.tar.gz (204.7 MB)\n",
      "     |████████████▍                   | 79.4 MB 33 kB/s eta 1:02:01  \u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 519, in read\n",
      "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
      "  File \"/opt/conda/lib/python3.9/http/client.py\", line 462, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"/opt/conda/lib/python3.9/http/client.py\", line 506, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"/opt/conda/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/opt/conda/lib/python3.9/ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/opt/conda/lib/python3.9/ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/cli/base_command.py\", line 164, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/cli/req_command.py\", line 205, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/commands/install.py\", line 338, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 92, in resolve\n",
      "    result = self._result = resolver.resolve(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 482, in resolve\n",
      "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 349, in resolve\n",
      "    self._add_to_criteria(self.state.criteria, r, parent=None)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 173, in _add_to_criteria\n",
      "    if not criterion.candidates:\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/resolvelib/structs.py\", line 151, in __bool__\n",
      "    return bool(self._sequence)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 155, in __bool__\n",
      "    return any(self)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 143, in <genexpr>\n",
      "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 47, in _iter_built\n",
      "    candidate = func()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/factory.py\", line 201, in _make_candidate_from_link\n",
      "    self._link_candidate_cache[link] = LinkCandidate(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 281, in __init__\n",
      "    super().__init__(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 156, in __init__\n",
      "    self.dist = self._prepare()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 225, in _prepare\n",
      "    dist = self._prepare_distribution()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 292, in _prepare_distribution\n",
      "    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/operations/prepare.py\", line 482, in prepare_linked_requirement\n",
      "    return self._prepare_linked_requirement(req, parallel_builds)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/operations/prepare.py\", line 527, in _prepare_linked_requirement\n",
      "    local_file = unpack_url(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/operations/prepare.py\", line 213, in unpack_url\n",
      "    file = get_http_url(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/operations/prepare.py\", line 94, in get_http_url\n",
      "    from_path, content_type = download(link, temp_dir.path)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/network/download.py\", line 145, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/cli/progress_bars.py\", line 144, in iter\n",
      "    for x in it:\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/network/utils.py\", line 63, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 576, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 541, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"/opt/conda/lib/python3.9/contextlib.py\", line 137, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 443, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install pyspark==3.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2058c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:0.8.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.databricks.delta.schema.autoMerge.enabled\",\"true\") \\\n",
    "    .config(\"spark.databricks.delta.autoOptimize.optimizeWrite\",\"true\") \\\n",
    "    .config(\"spark.databricks.delta.optimizeWrite.enabled\",\"true\") \\\n",
    "    .config(\"spark.databricks.delta.vacuum.parallelDelete.enabled\",\"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f18af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b68a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'tmp/sample.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea33f209",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf tmp/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd74d293",
   "metadata": {},
   "source": [
    "**Criando uma tabela Delta**\n",
    "\n",
    "Para criar uma tabela Delta, escreva um DataFrame no formato delta. Você pode usar o código Spark SQL existente e alterar o formato de parquet, csv, json e assim por diante para delta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1323650c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame(\n",
    "    [\n",
    "        (1,    100.0, 'registro da primeira linha', 'batata'),\n",
    "        (2,    150.0, 'registro da segunda linha', 'arroz'),\n",
    "    ],\n",
    "        ['id', 'number', 'txt', 'etc']\n",
    ")\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8851351",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_star_wars = spark.createDataFrame(\n",
    "    [\n",
    "        (1, 'Luke Skywalker', 1.72,'azul','19BBY','masculino','Tatooine','Humano'),\n",
    "        (2, 'C-3PO',1.67,'amarelo','112BBY','NA','Tatooine','Droid'),\n",
    "        (3, 'R2-D2', 0.67, 'vermelho','33BBY','NA','Naboo','Droid'),\n",
    "        (4, 'Anakin Skywalker', 1.88, 'azul','41.9BBY','masculino','Tatooine','Humano'),\n",
    "        (5, 'Leia Organa', 1.50,'castanho','19BBY','feminino','Alderaan','Humano'),\n",
    "        (6, 'Han Solo', 1.80, 'castanho', '29BBY', 'masculino', 'Corellia', 'Humano')\n",
    "          \n",
    "    ],\n",
    "        ['id', 'nome', 'altura', 'cor_dos_olhos','data_nascimento','sexo','planeta']\n",
    ")\n",
    "\n",
    "#df_star_wars.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab1ec72",
   "metadata": {},
   "source": [
    "### Referências:\n",
    "\n",
    "[1] [Data Lakehouse](https://databricks.com/glossary/data-lakehouse)\n",
    "\n",
    "[2] [Lakehouse: A New Generation of Open Platforms that Unify\n",
    "Data Warehousing and Advanced Analytics](https://databricks.com/wp-content/uploads/2020/12/cidr_lakehouse.pdf)\n",
    "\n",
    "[3] [Building a Data Lakehouse on GCP](https://services.google.com/fh/files/misc/building-a-data-lakehouse.pdf)\n",
    "\n",
    "[4] [Lakehouse: unindo o Data Lake e o Data Warehouse](https://medium.com/data-hackers/lakehouse-unindo-o-data-lake-e-o-data-warehouse-1428be2dda21)\n",
    "\n",
    "[5] [Construindo Data Lakehouse e muito mais, no Grupo Boticário — Data Hackers Podcast 44](https://medium.com/data-hackers/construindo-data-lakehouse-e-muito-mais-no-grupo-botic%C3%A1rio-data-hackers-podcast-44-20d67f05cfa4)\n",
    "\n",
    "[6] [What Is a Lakehouse?](https://databricks.com/blog/2020/01/30/what-is-a-data-lakehouse.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00274beb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a3122a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
